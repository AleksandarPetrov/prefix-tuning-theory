{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.trainer import Trainer, PrefixTrainer\n",
    "from mingpt.model import GPT\n",
    "from mingpt.data_tools import CustomDataset, eval, batch_end_callback, attention_visualization, label_batch\n",
    "import mingpt.data_tools as tasks\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that prefix-tuning cannot learn a new task.\n",
    "However, we hypothesize that if the model has been pre-trained on various tasks, prefix-tuning can elucidate one of them.\n",
    "In this notebook we demonstrate that this is the case.\n",
    "We will pre-train a model on four different tasks: sort in ascending and descending order, or add one or two to each element of the list.\n",
    "The model will recieve no indication of which task it needs to solve so it will learn to put approximately equal probability on all 4 completions, resulting in about 25% accuracyt.\n",
    "However, by learning prefixes, we can constrain the output distribution to only one of the tasks, thus demonstrating that while prefix-tuning cannot learn a completely new task, it can specialize a model to one of the tasks it has already seen.\n",
    "We will also show that the exact same method fails to learn a different new task (double histogram) which requires a novel attention pattern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's prepare our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Sample from the ascending dataset:\n",
      "   6   2   7   6   7   5   3   6   6  10 ->  10   7   7   6   6   6   6   5   3   2\n",
      "> Sample from the descending dataset:\n",
      "   1   5  10   3   2  10   3   3  10   5 ->   1   2   3   3   3   5   5  10  10  10\n",
      "> Sample from the InverseBinary dataset:\n",
      "   1   0   0   0   1   1   0   0   0   0 ->   0   1   1   1   0   0   1   1   1   1\n",
      "> Sample from the Add1 dataset:\n",
      "   2   8   8   2   3   4   3   2   3   8 ->   3   9   9   3   4   5   4   3   4   9\n",
      "> Sample from the Add2 dataset:\n",
      "   8   7   2   9   2   9   2   7  10   4 ->  10   9   4  11   4  11   4   9  12   6\n",
      "> Sample from the DoubleHistogram dataset:\n",
      "   5  10   6   3   8   7   7   8   2   8 ->   1   1   1   1   3   2   2   3   1   3\n",
      "> Sample from the Modulo dataset:\n",
      "   5   7   8   8   3   9   6   8   3   3 ->   0   2   3   3   3   4   1   3   3   3\n",
      "> Sample from the LessThan dataset:\n",
      "   5   4   7   3   4   2   2   1   3   6 ->   0   4   0   3   4   2   2   1   3   0\n",
      "> Sample from the MoreThanEqual dataset:\n",
      "   3   1   3   1   3   2   6   2   6   8 ->   3   0   3   0   3   0   6   0   6   8\n",
      "> Sample from the Divisible dataset:\n",
      "   4   8  10   2   3   4   4   4   4   8 ->   4   8   0   0   0   4   4   4   4   8\n",
      "> Sample from the NotDivisible dataset:\n",
      "   5   9   6   7   5   9   5   6   4   2 ->   0   9   6   7   0   9   0   6   4   2\n",
      "> Sample from the FilterAtLeastNTimes dataset:\n",
      "   3  10   3   3   5   4   4   2   1   5 ->   3   0   3   3   0   0   0   0   0   0\n",
      "> Sample from the ascending + add dataset:\n",
      "   2   9   3   6   1   5   8  10   6   4 ->   2   3   4   5   6   7   7   9  10  11\n",
      "> Sample from the add + less than dataset:\n",
      "   9  10  10   7   7   6   3   5   6   2 ->   0   0   0   8   8   7   4   6   7   3\n",
      "> Sample from the less than + add dataset:\n",
      "   6   8   1   4   6   7   5   6   8   1 ->   1   1   2   5   1   1   6   1   1   2\n",
      "> Sample from the less than + ascending dataset:\n",
      "   4   9  10   2   4   5   5   9   2   6 ->   0   0   0   0   0   0   0   0   2   2\n",
      "> Sample from the ascending + divisible dataset:\n",
      "   7  10   8   5   3   5  10   1   5   8 ->   0   0   0   0   0   1   3   5   5   5\n",
      "> Samples from the random training dataset:\n",
      "   2   2   8  10   4   6   8   1   5   3 ->   1   2   2   3   4   5   6   8   8  10\n",
      "   7   9  10   1   4   7   7  10   3   2 ->  10  10   9   7   7   7   4   3   2   1\n",
      "   9   3   4   4   1   8   7   9   5   7 ->  10   4   5   5   2   9   8  10   6   8\n",
      "   2   3   7   5   8   5   1   4   5   6 ->   0   1   1   1   0   1   1   0   1   0\n",
      "   8   5   4   4   1   6  10   1   7   8 ->   0   5   4   4   1   6   0   1   7   0\n",
      "   2   8   4   6   3   5   5   1   4  10 ->   2   8   4   6   0   0   0   0   4  10\n",
      "   0   0   0   0   1   1   0   1   1   1 ->   1   1   1   1   0   0   1   0   0   0\n",
      "   9   7   6  10   5   3   8   6   6   4 ->   3   4   5   6   6   6   7   8   9  10\n",
      "   4   2   3   8   6   5   5   5  10   6 ->  10   8   6   6   5   5   5   4   3   2\n",
      "  10   8   3   3   2   2   6   2   3   8 ->  11   9   4   4   3   3   7   3   4   9\n"
     ]
    }
   ],
   "source": [
    "prefix_size = 0\n",
    "\n",
    "print(\"> Sample from the ascending dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.SortDescending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the descending dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.SortAscending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the InverseBinary dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.InverseBinary()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Add1 dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Add2 dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Add1()+tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the DoubleHistogram dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.DoubleHistogram()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Modulo dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Modulo()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the LessThan dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the MoreThanEqual dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.MoreThanEqual()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Divisible dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Divisible()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the NotDivisible dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.NotDivisible()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the FilterAtLeastNTimes dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.FilterAtLeastNTimes()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the ascending + add dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.SortAscending()+tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the add + less than dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Add1()+tasks.LessThan()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the less than + add dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()+tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the less than + ascending dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()+tasks.SortAscending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the ascending + divisible dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()+tasks.SortAscending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "\n",
    "print(\"> Samples from the random training dataset:\")\n",
    "training_tasks = [\n",
    "    tasks.SortAscending(),\n",
    "    tasks.SortDescending(),\n",
    "    tasks.Add1(),\n",
    "    tasks.Modulo(),\n",
    "    tasks.LessThan(),\n",
    "    tasks.Divisible(),\n",
    "    tasks.InverseBinary(),\n",
    "]\n",
    "\n",
    "testing_tasks = [\n",
    "    tasks.MoreThanEqual(),\n",
    "    tasks.NotDivisible(),\n",
    "    tasks.DoubleHistogram(),\n",
    "    tasks.FilterAtLeastNTimes(),\n",
    "    tasks.SortAscending()+tasks.Add1(),\n",
    "    tasks.Add1() + tasks.LessThan(),\n",
    "    tasks.LessThan() + tasks.Add1(),\n",
    "    tasks.LessThan() + tasks.SortAscending(),\n",
    "    tasks.Divisible() + tasks.Add1(),\n",
    "]\n",
    "\n",
    "train_dataset_random = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=training_tasks)\n",
    "for i in range(10):\n",
    "    x, y = train_dataset_random[i]\n",
    "    print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_size = 10\n",
    "train_dataset_random = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=training_tasks)\n",
    "test_dataset_random = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=training_tasks)\n",
    "\n",
    "train_dataset_SortAscending = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortAscending()])\n",
    "train_dataset_SortDescending = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortDescending()])\n",
    "train_dataset_Add1 = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Add1()])\n",
    "train_dataset_Modulo = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Modulo()])\n",
    "train_dataset_LessThan = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.LessThan()])\n",
    "train_dataset_Divisible = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Divisible()])\n",
    "train_dataset_InverseBinary = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.InverseBinary()])\n",
    "\n",
    "test_dataset_SortAscending = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortAscending()])\n",
    "test_dataset_SortDescending = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortDescending()])\n",
    "test_dataset_Add1 = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Add1()])\n",
    "test_dataset_Modulo = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Modulo()])\n",
    "test_dataset_LessThan = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.LessThan()])\n",
    "test_dataset_Divisible = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Divisible()])\n",
    "test_dataset_InverseBinary = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.InverseBinary()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pre-train the model on `train_dataset_random` and check its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 37.85M\n",
      "Loading weights from cache, won't train from scratch.\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None\n",
    "model_config.vocab_size = train_dataset_random.get_vocab_size()\n",
    "model_config.block_size = train_dataset_random.get_block_size()\n",
    "model_config.n_layer = 12\n",
    "model_config.n_head = 8\n",
    "model_config.n_embd = 512\n",
    "model_config.batch_size = 512\n",
    "model = GPT(model_config)\n",
    "\n",
    "fname = '06_pretrained.pth'\n",
    "if os.path.exists(fname):\n",
    "    print(\"Loading weights from cache, won't train from scratch.\")\n",
    "    model.load_state_dict(torch.load(fname))\n",
    "    model.config = model_config\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    # create a Trainer object\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = 5e-4\n",
    "    train_config.max_iters = 100000\n",
    "    train_config.num_workers = 0\n",
    "    trainer = Trainer(train_config, model, train_dataset_random)\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    device = trainer.device\n",
    "\n",
    "    # save the model weights:\n",
    "    torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on a randomly labeled dataset:\n",
      "Final score: 1202/3200 = 37.56% correct\n",
      "Performance on the ascending dataset:\n",
      "Final score: 342/3200 = 10.69% correct\n",
      "Performance on the descending dataset:\n",
      "Final score: 666/3200 = 20.81% correct\n",
      "Performance on the add1 dataset:\n",
      "Final score: 799/3200 = 24.97% correct\n",
      "Performance on the Divisible dataset:\n",
      "Final score: 817/3200 = 25.53% correct\n",
      "Performance on the LessThan dataset:\n",
      "Final score: 147/3200 = 4.59% correct\n",
      "Performance on the Modulo dataset:\n",
      "Final score: 2360/3200 = 73.75% correct\n",
      "Performance on the InverseBinary dataset:\n",
      "Final score: 3200/3200 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance on a randomly labeled dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_random, device=device, max_batches=32)\n",
    "print(\"Performance on the ascending dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_SortAscending, device=device, max_batches=32)\n",
    "print(\"Performance on the descending dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_SortDescending, device=device, max_batches=32)\n",
    "print(\"Performance on the add1 dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_Add1, device=device, max_batches=32)\n",
    "print(\"Performance on the Divisible dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_Divisible, device=device, max_batches=32)\n",
    "print(\"Performance on the LessThan dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_LessThan, device=device, max_batches=32)\n",
    "print(\"Performance on the Modulo dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_Modulo, device=device, max_batches=32)\n",
    "print(\"Performance on the InverseBinary dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_InverseBinary, device=device, max_batches=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is randomness involved, the model might not have 25% accuracy for all tasks, but it still does have about 25% accuracy overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a separate prefix for each task now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix SortAscending loaded from cache.\n",
      "Final score: 3198/3200 = 99.94% correct\n",
      "\n",
      "Prefix SortDescending loaded from cache.\n",
      "Final score: 3196/3200 = 99.88% correct\n",
      "\n",
      "Prefix Add1 loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix Modulo loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix LessThan loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix Divisible loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix InverseBinary loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "TRAINING A PREFIX FOR THE MORETHANEQUAL TASK:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train_dataset_MoreThanEqual'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m train_config\u001b[38;5;241m.\u001b[39mmax_iters \u001b[38;5;241m=\u001b[39m iterations\n\u001b[1;32m     21\u001b[0m train_config\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m---> 22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PrefixTrainer(train_config, model, \u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_dataset_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, prefixes[task_name])\n\u001b[1;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[1;32m     24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_dataset_MoreThanEqual'"
     ]
    }
   ],
   "source": [
    "prefixes = dict()\n",
    "\n",
    "tasks = training_tasks+testing_tasks\n",
    "\n",
    "for task, iterations, lr in zip(\n",
    "    tasks,\n",
    "    [20_000]*len(tasks),\n",
    "    [5e-5]*len(tasks)\n",
    "):\n",
    "    task_name = str(task)\n",
    "    fname = f'06_prefix_{task_name}.pth'\n",
    "    train_dataset = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[task])\n",
    "    test_dataset = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[task])\n",
    "    if os.path.exists(fname):\n",
    "        prefixes[task_name] = torch.load(fname)\n",
    "        print(f\"Prefix {task} loaded from cache.\")\n",
    "    else:\n",
    "        print(f\"TRAINING A PREFIX FOR THE {task_name.upper()} TASK:\")\n",
    "        prefixes[task_name]  = torch.randn((model.config.n_layer,prefix_size, model.config.n_embd), requires_grad=True, device=device)\n",
    "        train_config = Trainer.get_default_config()\n",
    "        train_config.num_workers = 0\n",
    "        train_config.max_iters = iterations\n",
    "        train_config.learning_rate = lr\n",
    "        trainer = PrefixTrainer(train_config, model, train_dataset, prefixes[task_name])\n",
    "        trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "        trainer.run()\n",
    "        torch.save(prefixes[task_name], fname)\n",
    "    _ = eval(model, dataset=test_dataset, device=device, max_batches=32, prefixes=prefixes[task_name])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the task-specific prefixes, the accuracy gets close to 100% for all four tasks, as before.\n",
    "Furthermore, we obtained high accuracy for the task of ascending and incrementing by one even though the pretrained model had never seen it before. \n",
    "This was likely successful because of this new task being a composition of two pretraining tasks.\n",
    "In contrast, double histogram, another new task but one that is not a composition of pretraining tasks, cannot be solved with prefix tuning.\n",
    "This further illustrates that prefix-tuning is unlikely to be able to learn a completely new task but is able to elicit a pretraining task or to learn a new task that can be solved with skills learned during pre-training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
