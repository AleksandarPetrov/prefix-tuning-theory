{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.trainer import Trainer, PrefixTrainer\n",
    "from mingpt.model import GPT\n",
    "from mingpt.data_tools import CustomDataset, eval, batch_end_callback, attention_visualization, label_batch\n",
    "import mingpt.data_tools as tasks\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that prefix-tuning cannot learn a new task.\n",
    "However, we hypothesize that if the model has been pre-trained on various tasks, prefix-tuning can elucidate one of them.\n",
    "In this notebook we demonstrate that this is the case.\n",
    "We will pre-train a model on four different tasks: sort in ascending and descending order, or add one or two to each element of the list.\n",
    "The model will recieve no indication of which task it needs to solve so it will learn to put approximately equal probability on all 4 completions, resulting in about 25% accuracyt.\n",
    "However, by learning prefixes, we can constrain the output distribution to only one of the tasks, thus demonstrating that while prefix-tuning cannot learn a completely new task, it can specialize a model to one of the tasks it has already seen.\n",
    "We will also show that the exact same method fails to learn a different new task (double histogram) which requires a novel attention pattern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's prepare our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Sample from the ascending dataset:\n",
      "   5   4   1   2   5  10   4   9   4   3 ->  10   9   5   5   4   4   4   3   2   1\n",
      "> Sample from the descending dataset:\n",
      "   5   3   3   9   4  10   7   5   8   1 ->   1   3   3   4   5   5   7   8   9  10\n",
      "> Sample from the InverseBinary dataset:\n",
      "   0   1   1   0   0   1   1   0   0   0 ->   1   0   0   1   1   0   0   1   1   1\n",
      "> Sample from the Add1 dataset:\n",
      "   3   5   6   8   6   3   6   3   5   7 ->   4   6   7   9   7   4   7   4   6   8\n",
      "> Sample from the Add2 dataset:\n",
      "   2   3   3   4   1   1   2   2  10   1 ->   4   5   5   6   3   3   4   4  12   3\n",
      "> Sample from the DoubleHistogram dataset:\n",
      "   8   7   4   2   8   7   9   5   8   1 ->   3   2   1   1   3   2   1   1   3   1\n",
      "> Sample from the Modulo dataset:\n",
      "   3   1   4   7   6   8   4  10   4   8 ->   0   1   1   1   0   2   1   1   1   2\n",
      "> Sample from the LessThan dataset:\n",
      "   6   3   9   2   3   9   2   9  10   9 ->   0   3   0   2   3   0   2   0   0   0\n",
      "> Sample from the MoreThanEqual dataset:\n",
      "   4   1   4   9   5   4   2   5   3   1 ->   4   0   4   9   5   4   0   5   0   0\n",
      "> Sample from the Divisible dataset:\n",
      "   3   5   7   1   7   4   8   4   3   7 ->   3   0   0   0   0   0   0   0   3   0\n",
      "> Sample from the NotDivisible dataset:\n",
      "   3   4   6   5   6   1   9   3   2   4 ->   0   4   0   5   0   1   0   0   2   4\n",
      "> Sample from the FilterAtLeastNTimes dataset:\n",
      "   3   2  10   9   9   9   9  10   6   8 ->   0   0   0   9   9   9   9   0   0   0\n",
      "> Sample from the ascending + add dataset:\n",
      "   8   1   5   6   8  10   7   2   3   9 ->   2   3   4   6   7   8   9   9  10  11\n",
      "> Sample from the add + less than dataset:\n",
      "   6   4   4   2   6   7   8   8   5  10 ->   0   5   5   3   0   0   0   0   6   0\n",
      "> Sample from the less than + add dataset:\n",
      "   5   7  10   3   5   2   5  10   2  10 ->   1   1   1   4   1   3   1   1   3   1\n",
      "> Sample from the less than + ascending dataset:\n",
      "   1   4   2   6   1   9   8   4   2   6 ->   0   0   0   0   0   0   0   0   0   0\n",
      "> Sample from the ascending + divisible dataset:\n",
      "  10  10   5   5   5   1   5   8   7   5 ->   0   0   1   5   5   5   5   5   7   8\n",
      "> Samples from the random training dataset:\n",
      "   7   7  10   4   7   5   2   9   2   1 ->   1   2   2   4   5   7   7   7   9  10\n",
      "   5   1   5   7   7   2   1   1  10   7 ->  10   7   7   7   5   5   2   1   1   1\n",
      "   4   2   2   4   6   1   3   6   4  10 ->   5   3   3   5   7   2   4   7   5  11\n",
      "   2   7   2   2   6   2   7   3   3   3 ->   0   1   0   0   0   0   1   1   1   1\n",
      "   1   6   1   7   5   2   4   8   2   8 ->   0   0   0   0   0   0   0   0   0   0\n",
      "   3   8   1   1   6   6   3   2   2   9 ->   3   0   0   0   6   6   3   0   0   9\n",
      "   0   0   1   1   0   1   1   1   0   0 ->   1   1   0   0   1   0   0   0   1   1\n",
      "   9   7   7   3  10   7   1   7   3   9 ->   1   3   3   7   7   7   7   9   9  10\n",
      "   9   4   5   9   9   3   7  10   8   9 ->  10   9   9   9   9   8   7   5   4   3\n",
      "   1   6  10   2   1   9   2   1   1   5 ->   2   7  11   3   2  10   3   2   2   6\n"
     ]
    }
   ],
   "source": [
    "prefix_size = 0\n",
    "\n",
    "print(\"> Sample from the ascending dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.SortDescending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the descending dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.SortAscending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the InverseBinary dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.InverseBinary()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Add1 dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Add2 dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Add1()+tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the DoubleHistogram dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.DoubleHistogram()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Modulo dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Modulo()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the LessThan dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the MoreThanEqual dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.MoreThanEqual()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the Divisible dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Divisible()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the NotDivisible dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.NotDivisible()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the FilterAtLeastNTimes dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.FilterAtLeastNTimes()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the ascending + add dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.SortAscending()+tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the add + less than dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.Add1()+tasks.LessThan()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the less than + add dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()+tasks.Add1()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the less than + ascending dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()+tasks.SortAscending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "print(\"> Sample from the ascending + divisible dataset:\")\n",
    "x, y = CustomDataset('train', tasks=[tasks.LessThan()+tasks.SortAscending()], prefix_padding=prefix_size, num_digits=10)[0]\n",
    "print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))\n",
    "\n",
    "\n",
    "print(\"> Samples from the random training dataset:\")\n",
    "training_tasks = [\n",
    "    tasks.SortAscending(),\n",
    "    tasks.SortDescending(),\n",
    "    tasks.Add1(),\n",
    "    tasks.Modulo(),\n",
    "    tasks.LessThan(),\n",
    "    tasks.Divisible(),\n",
    "    tasks.InverseBinary(),\n",
    "]\n",
    "\n",
    "testing_tasks = [\n",
    "    tasks.Add1() + tasks.Add1(),\n",
    "    tasks.Add1() + tasks.Add1() + tasks.Add1(),\n",
    "    tasks.MoreThanEqual(),\n",
    "    tasks.NotDivisible(),\n",
    "    tasks.DoubleHistogram(),\n",
    "    tasks.FilterAtLeastNTimes(),\n",
    "    tasks.SortAscending()+tasks.Add1(),\n",
    "    tasks.Add1() + tasks.LessThan(),\n",
    "    tasks.LessThan() + tasks.Add1(),\n",
    "    tasks.LessThan() + tasks.SortAscending(),\n",
    "    tasks.Divisible() + tasks.Add1(),\n",
    "]\n",
    "\n",
    "train_dataset_random = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=training_tasks)\n",
    "for i in range(10):\n",
    "    x, y = train_dataset_random[i]\n",
    "    print(\"\".join(map(lambda x: f\"{x:>4}\", x.tolist()[:10])) + \" ->\" + \"\".join(map(lambda x: f\"{x:>4}\", y.tolist()[9:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_size = 10\n",
    "train_dataset_random = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=training_tasks)\n",
    "test_dataset_random = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=training_tasks)\n",
    "\n",
    "train_dataset_SortAscending = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortAscending()])\n",
    "train_dataset_SortDescending = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortDescending()])\n",
    "train_dataset_Add1 = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Add1()])\n",
    "train_dataset_Modulo = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Modulo()])\n",
    "train_dataset_LessThan = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.LessThan()])\n",
    "train_dataset_Divisible = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Divisible()])\n",
    "train_dataset_InverseBinary = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.InverseBinary()])\n",
    "\n",
    "test_dataset_SortAscending = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortAscending()])\n",
    "test_dataset_SortDescending = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.SortDescending()])\n",
    "test_dataset_Add1 = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Add1()])\n",
    "test_dataset_Modulo = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Modulo()])\n",
    "test_dataset_LessThan = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.LessThan()])\n",
    "test_dataset_Divisible = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.Divisible()])\n",
    "test_dataset_InverseBinary = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[tasks.InverseBinary()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pre-train the model on `train_dataset_random` and check its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 37.85M\n",
      "Loading weights from cache, won't train from scratch.\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None\n",
    "model_config.vocab_size = train_dataset_random.get_vocab_size()\n",
    "model_config.block_size = train_dataset_random.get_block_size()\n",
    "model_config.n_layer = 12\n",
    "model_config.n_head = 8\n",
    "model_config.n_embd = 512\n",
    "model_config.batch_size = 512\n",
    "model = GPT(model_config)\n",
    "\n",
    "fname = '06_pretrained.pth'\n",
    "if os.path.exists(fname):\n",
    "    print(\"Loading weights from cache, won't train from scratch.\")\n",
    "    model.load_state_dict(torch.load(fname))\n",
    "    model.config = model_config\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    # create a Trainer object\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = 5e-4\n",
    "    train_config.max_iters = 100000\n",
    "    train_config.num_workers = 0\n",
    "    trainer = Trainer(train_config, model, train_dataset_random)\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    device = trainer.device\n",
    "\n",
    "    # save the model weights:\n",
    "    torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on a randomly labeled dataset:\n",
      "Final score: 1191/3200 = 37.22% correct\n",
      "Performance on the ascending dataset:\n",
      "Final score: 326/3200 = 10.19% correct\n",
      "Performance on the descending dataset:\n",
      "Final score: 707/3200 = 22.09% correct\n",
      "Performance on the add1 dataset:\n",
      "Final score: 774/3200 = 24.19% correct\n",
      "Performance on the Divisible dataset:\n",
      "Final score: 833/3200 = 26.03% correct\n",
      "Performance on the LessThan dataset:\n",
      "Final score: 144/3200 = 4.50% correct\n",
      "Performance on the Modulo dataset:\n",
      "Final score: 2318/3200 = 72.44% correct\n",
      "Performance on the InverseBinary dataset:\n",
      "Final score: 3200/3200 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance on a randomly labeled dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_random, device=device, max_batches=32)\n",
    "print(\"Performance on the ascending dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_SortAscending, device=device, max_batches=32)\n",
    "print(\"Performance on the descending dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_SortDescending, device=device, max_batches=32)\n",
    "print(\"Performance on the add1 dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_Add1, device=device, max_batches=32)\n",
    "print(\"Performance on the Divisible dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_Divisible, device=device, max_batches=32)\n",
    "print(\"Performance on the LessThan dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_LessThan, device=device, max_batches=32)\n",
    "print(\"Performance on the Modulo dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_Modulo, device=device, max_batches=32)\n",
    "print(\"Performance on the InverseBinary dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_InverseBinary, device=device, max_batches=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is randomness involved, the model might not have 25% accuracy for all tasks, but it still does have about 25% accuracy overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a separate prefix for each task now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix SortAscending loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix SortDescending loaded from cache.\n",
      "Final score: 3199/3200 = 99.97% correct\n",
      "\n",
      "Prefix Add1 loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix Modulo loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix LessThan loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix Divisible loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix InverseBinary loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "Prefix Add1_Add1 loaded from cache.\n",
      "Final score: 3200/3200 = 100.00% correct\n",
      "\n",
      "TRAINING A PREFIX FOR THE ADD1_ADD1_ADD1 TASK:\n",
      "running on device cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m PrefixTrainer(train_config, model, train_dataset, prefixes[task_name])\n\u001b[1;32m     25\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(prefixes[task_name], fname)\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, dataset\u001b[38;5;241m=\u001b[39mtest_dataset, device\u001b[38;5;241m=\u001b[39mdevice, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, prefixes\u001b[38;5;241m=\u001b[39mprefixes[task_name])\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/minGPT/mingpt/trainer.py:165\u001b[0m, in \u001b[0;36mPrefixTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m logits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m    168\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/minGPT/mingpt/model.py:335\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets, prefixes, activations_record)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# CHANGED: passes through arguments\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh):\n\u001b[0;32m--> 335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefixes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivations_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivations_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    341\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/minGPT/mingpt/model.py:128\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, prefix, activations_record)\u001b[0m\n\u001b[1;32m    125\u001b[0m     input_for_attention \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# CHANGED: pass the optional activation record\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m attn_block_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_for_attention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivations_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivations_record\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# CHANGED: record the attention block output if desired\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m activations_record \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/minGPT/mingpt/model.py:70\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, activations_record)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m att \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 70\u001b[0m att \u001b[38;5;241m=\u001b[39m att\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     71\u001b[0m att \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(att, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# CHANGED: record the attention pattern\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "prefixes = dict()\n",
    "\n",
    "tasks = training_tasks+testing_tasks\n",
    "\n",
    "for task, iterations, lr in zip(\n",
    "    tasks,\n",
    "    [50_000]*len(tasks),\n",
    "    [5e-5]*len(tasks)\n",
    "):\n",
    "    task_name = str(task)\n",
    "    fname = f'06_prefix_{task_name}.pth'\n",
    "    train_dataset = CustomDataset('train', prefix_padding=prefix_size, num_digits=10, tasks=[task])\n",
    "    test_dataset = CustomDataset('test', prefix_padding=prefix_size, num_digits=10, tasks=[task])\n",
    "    if os.path.exists(fname):\n",
    "        prefixes[task_name] = torch.load(fname)\n",
    "        print(f\"Prefix {task} loaded from cache.\")\n",
    "    else:\n",
    "        print(f\"TRAINING A PREFIX FOR THE {task_name.upper()} TASK:\")\n",
    "        prefixes[task_name]  = torch.randn((model.config.n_layer,prefix_size, model.config.n_embd), requires_grad=True, device=device)\n",
    "        train_config = Trainer.get_default_config()\n",
    "        train_config.num_workers = 0\n",
    "        train_config.max_iters = iterations\n",
    "        train_config.learning_rate = lr\n",
    "        trainer = PrefixTrainer(train_config, model, train_dataset, prefixes[task_name])\n",
    "        trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "        trainer.run()\n",
    "        torch.save(prefixes[task_name], fname)\n",
    "    _ = eval(model, dataset=test_dataset, device=device, max_batches=32, prefixes=prefixes[task_name])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the task-specific prefixes, the accuracy gets close to 100% for all four tasks, as before.\n",
    "Furthermore, we obtained high accuracy for the task of ascending and incrementing by one even though the pretrained model had never seen it before. \n",
    "This was likely successful because of this new task being a composition of two pretraining tasks.\n",
    "In contrast, double histogram, another new task but one that is not a composition of pretraining tasks, cannot be solved with prefix tuning.\n",
    "This further illustrates that prefix-tuning is unlikely to be able to learn a completely new task but is able to elicit a pretraining task or to learn a new task that can be solved with skills learned during pre-training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
