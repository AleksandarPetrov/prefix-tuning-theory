{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.trainer import Trainer, PrefixTrainer, LoRATrainer\n",
    "from mingpt.model import GPT\n",
    "from mingpt.data_tools import CustomDataset, eval, batch_end_callback, attention_visualization, label_batch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List\n",
    "\n",
    "from minlora import get_lora_state_dict\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_size = 12\n",
    "train_dataset_random = CustomDataset('train', mode=\"random\", prefix_padding=prefix_size)\n",
    "train_dataset_ascending = CustomDataset('train', mode=\"ascending\", prefix_padding=prefix_size)\n",
    "train_dataset_descending = CustomDataset('train', mode=\"descending\", prefix_padding=prefix_size)\n",
    "train_dataset_add1 = CustomDataset('train', mode=\"add1\", prefix_padding=prefix_size)\n",
    "train_dataset_add2 = CustomDataset('train', mode=\"add2\", prefix_padding=prefix_size)\n",
    "train_dataset_ascending_add1 = CustomDataset('train', mode=\"ascending_add1\", prefix_padding=prefix_size)\n",
    "train_dataset_double_hist = CustomDataset('train', mode=\"double_hist\", prefix_padding=prefix_size)\n",
    "\n",
    "test_dataset_random = CustomDataset('test', mode=\"random\", prefix_padding=prefix_size)\n",
    "test_dataset_ascending = CustomDataset('test', mode=\"ascending\", prefix_padding=prefix_size)\n",
    "test_dataset_descending = CustomDataset('test', mode=\"descending\", prefix_padding=prefix_size)\n",
    "test_dataset_add1 = CustomDataset('test', mode=\"add1\", prefix_padding=prefix_size)\n",
    "test_dataset_add2 = CustomDataset('test', mode=\"add2\", prefix_padding=prefix_size)\n",
    "test_dataset_ascending_add1 = CustomDataset('test', mode=\"ascending_add1\", prefix_padding=prefix_size)\n",
    "test_dataset_double_hist = CustomDataset('test', mode=\"double_hist\", prefix_padding=prefix_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained model (if you don't have it, run notebook 03 first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.17M\n",
      "Loading weights from cache, won't train from scratch.\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None\n",
    "model_config.vocab_size = train_dataset_random.get_vocab_size()\n",
    "model_config.block_size = train_dataset_random.get_block_size()\n",
    "model_config.n_layer = 4\n",
    "model_config.n_head = 4\n",
    "model_config.n_embd = 256\n",
    "model_config.batch_size = 512\n",
    "model = GPT(model_config)\n",
    "\n",
    "fname = '04_pretrained.pth'\n",
    "if os.path.exists(fname):\n",
    "    print(\"Loading weights from cache, won't train from scratch.\")\n",
    "    model.load_state_dict(torch.load(fname))\n",
    "    model.config = model_config\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    # create a Trainer object\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = 5e-4\n",
    "    train_config.max_iters = 40000\n",
    "    train_config.num_workers = 0\n",
    "    trainer = Trainer(train_config, model, train_dataset_random)\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    device = trainer.device\n",
    "\n",
    "    # save the model weights:\n",
    "    torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the pretrained model has zero accuracy at double histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained performance on the double_hist dataset:\n",
      "Final score: 0/3200 = 0.00% correct\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrained performance on the double_hist dataset:\")\n",
    "_ = eval(model, dataset=test_dataset_double_hist, device=device, max_batches=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prefix for this task still has close to 0% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix double_histloaded from cache.\n",
      "Performance on the double_hist dataset with prefix:\n",
      "Final score: 10/3200 = 0.31% correct\n"
     ]
    }
   ],
   "source": [
    "fname = f'04_prefix_double_hist.pth'\n",
    "if os.path.exists(fname):\n",
    "    prefix = torch.load(fname)\n",
    "    print(f\"Prefix double_histloaded from cache.\")\n",
    "else:\n",
    "    prefix  = torch.randn((model.config.n_layer,prefix_size, model.config.n_embd), requires_grad=True, device=device)\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.num_workers = 0\n",
    "    train_config.max_iters = 100_000\n",
    "    train_config.learning_rate = 5e-5\n",
    "    trainer = PrefixTrainer(train_config, model, train_dataset_double_hist, prefix)\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    torch.save(prefix, fname)\n",
    "print(\"Performance on the double_hist dataset with prefix:\")\n",
    "_ = eval(model, dataset=test_dataset_double_hist, device=device, max_batches=32, prefixes=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, rank 1 LoRA update of the MLP weights for just a tenth of the training iterations results in high accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n",
      "iter_dt  14.36ms; iter   1000: train loss 1.07757\n",
      "iter_dt  14.93ms; iter   2000: train loss 1.02382\n",
      "iter_dt  14.35ms; iter   3000: train loss 0.95937\n",
      "iter_dt  16.79ms; iter   4000: train loss 0.84579\n",
      "iter_dt  14.31ms; iter   5000: train loss 0.55840\n",
      "iter_dt  16.43ms; iter   6000: train loss 0.18979\n",
      "iter_dt  14.44ms; iter   7000: train loss 0.08735\n",
      "iter_dt  14.87ms; iter   8000: train loss 0.07735\n",
      "iter_dt  14.52ms; iter   9000: train loss 0.06246\n",
      "iter_dt  16.38ms; iter  10000: train loss 0.08361\n",
      "Final score: 2956/3200 = 92.38% correct\n"
     ]
    }
   ],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.num_workers = 0\n",
    "train_config.max_iters = 10_000\n",
    "train_config.learning_rate = 5e-3\n",
    "trainer = LoRATrainer(\n",
    "    train_config, \n",
    "    model, \n",
    "    train_dataset_double_hist, \n",
    "    rank=1,\n",
    "    device=device,\n",
    ")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer.run()\n",
    "_ = eval(model, test_dataset_double_hist, device=device, max_batches=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is despite the two fine-tuning approaches having the same number of learnable parameters. The limitated performance of prefix-tuning is not simply because of it using few parameters as LoRA with the same number of parameters solves the task. Therefore, prefix-tuning (and prompting) suffer unique structural limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LoRAparameters: 12288\n",
      "Number of prefix parameters: 12288 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of LoRAparameters: {sum(p.numel() for p in get_lora_state_dict(model).values())}\")\n",
    "print(f\"Number of prefix parameters: {torch.numel(prefix)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
