{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.trainer import Trainer, PrefixTrainer, LoRATrainer\n",
    "from mingpt.model import GPT\n",
    "from mingpt.data_tools import CustomDataset, eval, batch_end_callback, attention_visualization, label_batch, eval_memory\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Literal\n",
    "\n",
    "from minlora import get_lora_state_dict, remove_lora\n",
    "\n",
    "seed = 1234\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "class WordsDataset:\n",
    "    def __init__(\n",
    "            self, \n",
    "            file: str, \n",
    "            src_lang: Literal[\"AmE\", \"BrE\", \"DE\", \"EP\", \"ES\"], \n",
    "            tgt_lang: Literal[\"AmE\", \"BrE\", \"DE\", \"EP\", \"ES\"],\n",
    "            prefix_padding: int = 0,\n",
    "        ):\n",
    "\n",
    "        self.src_column = f\"{src_lang}_wordform\"\n",
    "        self.tgt_column = f\"{tgt_lang}_wordform\"\n",
    "        \n",
    "        self.prefix_padding = prefix_padding\n",
    "\n",
    "        self.df = pd.read_csv(file)\n",
    "        # remove empty rows\n",
    "        self.df = self.df[self.df[self.src_column].notna()]\n",
    "        # if multiple same entries exist for the src_column, keep only the first\n",
    "        self.df = self.df.drop_duplicates(subset=[self.src_column], keep=\"first\")\n",
    "\n",
    "        wordform_columns = [col for col in self.df.columns if col.endswith(\"_wordform\")]\n",
    "        normalize = lambda str: unicodedata.normalize('NFD', str).encode('ASCII', 'ignore').decode()\n",
    "        # aplly normalize to each column in wordform_columns\n",
    "        for col in wordform_columns:\n",
    "            self.df[col] = self.df[col].apply(normalize)\n",
    "\n",
    "        # get the unique characters\n",
    "        self.chars = sorted(list(set( \"\".join([\"\".join(self.df[col].tolist()) for col in wordform_columns ]) )))\n",
    "        self.chars.append(\"<TR>\")\n",
    "        self.chars.append(\"<PAD>\")\n",
    "        self.tokenizer_map = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.detokenizer_map = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.detokenizer_map.update({-1: \"<M>\"})\n",
    "\n",
    "        # longest word\n",
    "        self.longest = max([max([len(w) for w in self.df[self.src_column]]) for col in wordform_columns])\n",
    "    \n",
    "    def detokenize(self, toks: List[int]):\n",
    "        if isinstance(toks, torch.Tensor):\n",
    "            toks = toks.tolist()\n",
    "        return \"\".join([self.detokenizer_map[x] for x in toks])\n",
    "    \n",
    "    def tokenize(self, str: str):\n",
    "        return [self.tokenizer_map[s] for s in str]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return 2+2*self.longest+self.prefix_padding\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenize(self.df.iloc[idx][self.src_column])\n",
    "        tgt = self.tokenize(self.df.iloc[idx][self.tgt_column])\n",
    "        content_length = len(src)+len(tgt)+self.prefix_padding+1\n",
    "        cat = torch.tensor(\n",
    "            [0 for _ in range(self.prefix_padding)] + src + [self.tokenizer_map[\"<TR>\"]] + tgt + [self.tokenizer_map[\"<PAD>\"] for _ in range(self.get_block_size()-content_length)],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input and prefix locations\n",
    "        y[:len(src)+1+self.prefix_padding-1] = -1\n",
    "        y[len(src)+1+self.prefix_padding-1+len(tgt)+1:] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4921\n",
      "a \t-- <M>\n",
      "n \t-- <M>\n",
      "a \t-- <M>\n",
      "l \t-- <M>\n",
      "o \t-- <M>\n",
      "g \t-- <M>\n",
      "y \t-- <M>\n",
      "<TR> \t-- A\n",
      "A \t-- n\n",
      "n \t-- a\n",
      "a \t-- l\n",
      "l \t-- o\n",
      "o \t-- g\n",
      "g \t-- i\n",
      "i \t-- e\n",
      "e \t-- <PAD>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n",
      "<PAD> \t-- <M>\n"
     ]
    }
   ],
   "source": [
    "ds = WordsDataset(\"PHOR_in_One_LDB.csv\", src_lang=\"BrE\", tgt_lang=\"DE\")\n",
    "print(f\"Dataset size: {len(ds)}\")\n",
    "idx = 165\n",
    "for x, y in zip(ds[idx][0], ds[idx][1]):\n",
    "    print(f\"{ds.detokenize([x.item()])} \\t-- {ds.detokenize([y.item()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_size = 66\n",
    "dataset_en_de = WordsDataset(\"PHOR_in_One_LDB.csv\", src_lang=\"BrE\", tgt_lang=\"DE\", prefix_padding=prefix_size)\n",
    "dataset_en_es = WordsDataset(\"PHOR_in_One_LDB.csv\", src_lang=\"BrE\", tgt_lang=\"ES\", prefix_padding=prefix_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained model (if you don't have it, run notebook 03 first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.20M\n",
      "Loading weights from cache (05_1234_pretrained.pth), won't train from scratch.\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None\n",
    "model_config.vocab_size = dataset_en_de.get_vocab_size()\n",
    "model_config.block_size = dataset_en_de.get_block_size()\n",
    "model_config.n_layer = 4\n",
    "model_config.n_head = 4\n",
    "model_config.n_embd = 256\n",
    "model_config.batch_size = 512\n",
    "model = GPT(model_config)\n",
    "\n",
    "fname = f'05_{seed}_pretrained.pth'\n",
    "remove_lora(model)\n",
    "if os.path.exists(fname):\n",
    "    print(f\"Loading weights from cache ({fname}), won't train from scratch.\")\n",
    "    model.load_state_dict(torch.load(fname))\n",
    "    model.config = model_config\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    # create a Trainer object\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = 5e-4\n",
    "    train_config.max_iters = 50000\n",
    "    train_config.num_workers = 0\n",
    "    trainer = Trainer(train_config, model, dataset_en_de)\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    device = trainer.device\n",
    "\n",
    "    # save the model weights:\n",
    "    torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the pretrained model has zero accuracy at double histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pretrained performance on the EN-DE dataset:\")\n",
    "_ = eval_memory(model, dataset=dataset_en_de, device=device, show_wrong_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prefix for this task still has close to 0% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix 05_1234_prefix_spanish.pth from cache.\n"
     ]
    }
   ],
   "source": [
    "fname = f'05_{seed}_prefix_spanish.pth'\n",
    "remove_lora(model)\n",
    "if os.path.exists(fname):\n",
    "    prefix = torch.load(fname)\n",
    "    print(f\"Prefix {fname} from cache.\")\n",
    "else:\n",
    "    prefix  = torch.randn((model.config.n_layer,prefix_size, model.config.n_embd), requires_grad=True, device=device)\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.num_workers = 0\n",
    "    train_config.max_iters = 100_000\n",
    "    train_config.learning_rate = 5e-5\n",
    "    trainer = PrefixTrainer(train_config, model, dataset_en_es, prefix)\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    torch.save(prefix, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance on the EN-ES dataset with prefix:\")\n",
    "_ = eval_memory(model, dataset=dataset_en_es, device=device, show_wrong_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, rank 1 LoRA update of the MLP weights for just a tenth of the training iterations results in high accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m LoRATrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     train_config, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     model, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     where\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mc_attn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mc_proj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mc_fc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlm_head\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m trainer\u001b[39m.\u001b[39mset_callback(\u001b[39m'\u001b[39m\u001b[39mon_batch_end\u001b[39m\u001b[39m'\u001b[39m, batch_end_callback)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m _ \u001b[39m=\u001b[39m eval_memory(model, dataset\u001b[39m=\u001b[39mdataset_en_es, device\u001b[39m=\u001b[39mdevice, show_correct\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/minGPT/mingpt/trainer.py:258\u001b[0m, in \u001b[0;36mLoRATrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m     \u001b[39m# fetch the next batch (x, y) and re-init iterator if needed\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_iter)\n\u001b[1;32m    259\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m         data_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[idx][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_column])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49miloc[idx][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_column])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     content_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(src)\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(tgt)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefix_padding\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     cat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m         [\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefix_padding)] \u001b[39m+\u001b[39m src \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer_map[\u001b[39m\"\u001b[39m\u001b[39m<TR>\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m+\u001b[39m tgt \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer_map[\u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_block_size()\u001b[39m-\u001b[39mcontent_length)],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m         dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1658\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1658\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_ixs(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/pandas/core/frame.py:3652\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3650\u001b[0m \u001b[39m# irow\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 3652\u001b[0m     new_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mfast_xs(i)\n\u001b[1;32m   3654\u001b[0m     \u001b[39m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(new_mgr\u001b[39m.\u001b[39marray, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m new_mgr\u001b[39m.\u001b[39marray\u001b[39m.\u001b[39mbase \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1075\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m   1072\u001b[0m     \u001b[39m# Such assignment may incorrectly coerce NaT to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     \u001b[39m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m     \u001b[39mfor\u001b[39;00m i, rl \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(blk\u001b[39m.\u001b[39mmgr_locs):\n\u001b[0;32m-> 1075\u001b[0m         result[rl] \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49miget((i, loc))\n\u001b[1;32m   1077\u001b[0m \u001b[39mif\u001b[39;00m immutable_ea:\n\u001b[1;32m   1078\u001b[0m     dtype \u001b[39m=\u001b[39m cast(ExtensionDtype, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.num_workers = 0\n",
    "train_config.max_iters = 50_000\n",
    "train_config.learning_rate = 1e-3\n",
    "trainer = LoRATrainer(\n",
    "    train_config, \n",
    "    model, \n",
    "    dataset_en_es,\n",
    "    rank=4, #16 works\n",
    "    device=device,\n",
    "    where=[\"c_attn\", \"c_proj\", \"c_fc\", \"lm_head\"],\n",
    ")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer.run()\n",
    "_ = eval_memory(model, dataset=dataset_en_es, device=device, show_correct=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is despite the two fine-tuning approaches having the same number of learnable parameters. The limitated performance of prefix-tuning is not simply because of it using few parameters as LoRA with the same number of parameters solves the task. Therefore, prefix-tuning (and prompting) suffer unique structural limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prefix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m lora_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m get_lora_state_dict(model)\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of prefix parameters: \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mnumel(prefix)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btvg-box/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/minGPT/05_memory_capacity.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of LoRA parameters: \u001b[39m\u001b[39m{\u001b[39;00mlora_params\u001b[39m}\u001b[39;00m\u001b[39m (equivalent to \u001b[39m\u001b[39m{\u001b[39;00mlora_params\u001b[39m/\u001b[39m(model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer\u001b[39m*\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_embd)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m prefixes)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prefix' is not defined"
     ]
    }
   ],
   "source": [
    "lora_params = sum(p.numel() for p in get_lora_state_dict(model).values())\n",
    "\n",
    "print(f\"Number of prefix parameters: {torch.numel(prefix)} \")\n",
    "print(f\"Number of LoRA parameters: {lora_params} (equivalent to {lora_params/(model.config.n_layer*model.config.n_embd):.2f} prefixes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
