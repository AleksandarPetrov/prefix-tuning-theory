{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/cdt21/aleks/PUBLIC-prefix-tuning-theory/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.trainer import Trainer, PrefixTrainer, LoRATrainer\n",
    "from mingpt.model import GPT\n",
    "from mingpt.data_tools import eval_classification, batch_end_callback, attention_visualization, label_batch, save_checkpoint\n",
    "from mingpt.bpe import BPETokenizer\n",
    "\n",
    "from minlora import remove_lora\n",
    "\n",
    "# import Emotion dataset from parent directory\n",
    "from datasets import load_dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "from minlora import get_lora_state_dict\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping a dataset so that we embed the tokenziation and adding the prefix\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, dataset_identifier: str, split: str, prefix_size=0, separator=\" CLASS: \", max_len=64, token_permute_map: Dict[int, int] = None, **kwargs):\n",
    "        \n",
    "        if dataset_identifier == \"winogrande\" and split == \"test\":\n",
    "            split = \"validation\"\n",
    "\n",
    "        self.split = split\n",
    "        self.max_len = max_len\n",
    "        self.dataset_identifier = dataset_identifier\n",
    "        self.ds = load_dataset(self.dataset_identifier, split=self.split, **kwargs)\n",
    "        self.prefix_size = prefix_size\n",
    "\n",
    "        if self.dataset_identifier == \"dair-ai/emotion\":\n",
    "            self.labels = [s.replace(\"_\", \" \") for s in self.ds.features[\"label\"].names] # type: ignore\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "        self.tokenizer = BPETokenizer()\n",
    "        self.eos_token = self.tokenizer.encoder.encoder['<|endoftext|>']\n",
    "        self.separator_tokens = self.tokenizer(separator)[0]\n",
    "        self.prefix_size = prefix_size\n",
    "        self.prefix_tokens = torch.tensor([self.tokenizer.encoder.encoder['?']]*prefix_size, dtype=torch.long)\n",
    "\n",
    "        if token_permute_map is None:\n",
    "            self.token_permute_map = {i:i for i in range(50257)}\n",
    "        else:\n",
    "            self.token_permute_map = token_permute_map\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.ds.num_rows  # type: ignore\n",
    "    \n",
    "    def permute_tokens(self, tokens: torch.tensor) -> torch.tensor:\n",
    "        return tokens.clone().apply_(self.token_permute_map.get)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        if self.dataset_identifier == \"dair-ai/emotion\":\n",
    "            x, label_id = item[\"text\"], item[\"label\"]\n",
    "            label = self.labels[label_id]\n",
    "        elif self.dataset_identifier == \"race\":\n",
    "            x = \"QUESTION: \" + item[\"question\"] + \"\\nPOSSIBLE ANSWERS: \"\n",
    "            for i, option in zip(\"ABCD\", item[\"options\"]):\n",
    "                x += f\"{i}. {option} \"\n",
    "            x += \"\\nTEXT: \" + item[\"article\"] + \"\\n\"\n",
    "            label = item[\"answer\"]\n",
    "        elif self.dataset_identifier == \"winogrande\":\n",
    "            x =  f\"{item['sentence']} What is missing: {item['option1']} or {item['option2']}? \"\n",
    "            label = \" \" + (item[\"option1\"] if int(item[\"answer\"]) == 1 else item[\"option2\"])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        x_tok = self.tokenizer(x)[0]\n",
    "        label_tok = self.tokenizer(label)[0]\n",
    "\n",
    "        current_length = self.prefix_size + len(x_tok) + len(self.separator_tokens) + len(label_tok) + 1\n",
    "        to_trim = current_length - self.max_len\n",
    "\n",
    "        pad = []\n",
    "        if to_trim > 0:\n",
    "            x_tok = x_tok[:-to_trim]\n",
    "        elif to_trim < 0:\n",
    "            pad = [0]*(-to_trim)\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            xy = torch.hstack((self.prefix_tokens, x_tok, self.separator_tokens, label_tok, torch.tensor([self.eos_token]+pad, dtype=torch.long)))\n",
    "            x = self.permute_tokens(xy[:-1].clone())\n",
    "            y = self.permute_tokens(xy[1:].clone())\n",
    "\n",
    "            y[:self.prefix_size+len(x_tok)+len(self.separator_tokens)-1] = -1\n",
    "            if to_trim < 0:\n",
    "                y[to_trim:] = -1\n",
    "            return x, y\n",
    "\n",
    "        else:\n",
    "            test_x = torch.hstack((self.prefix_tokens, x_tok, self.separator_tokens))\n",
    "            test_y = torch.hstack((label_tok, torch.tensor([self.eos_token], dtype=torch.long)))\n",
    "            return self.permute_tokens(test_x), self.permute_tokens(test_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset winogrande (/homes/cdt21/aleks/.cache/huggingface/datasets/winogrande/winogrande_debiased/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   30,    30,    30,    30,    30, 29284,   373,   257,   881,  1365,\n",
      "        23923,   621, 14200,   523,  4808,  1464,  1392,   262,  4577,  2663,\n",
      "           13,  1867,   318,  4814,    25, 10490,   393, 14200,    30,   220,\n",
      "        44879, 23988,  3537, 17887,  1137,    25,   220]), tensor([14200, 50256]))\n",
      "['?????Sarah was a much better surgeon than Maria so _ always got the easier cases. What is missing: Sarah or Maria? CORRECT ANSWER: ', ' Maria<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "# rng = np.random.default_rng(12345)\n",
    "# permute_map = {x: y for x, y in zip(range(50257), rng.permutation(range(50257)))}\n",
    "\n",
    "# ds = Dataset(dataset_identifier=\"dair-ai/emotion\", split=\"test\", prefix_size=5, token_permute_map=permute_map)\n",
    "# print(ds[0])\n",
    "# def decode_masked(inp):\n",
    "#     if len(inp) == 0:\n",
    "#         return \"\"\n",
    "#     decoded = \"\"\n",
    "#     if inp[0] == -1:\n",
    "#         decoded+=\"[-1]\"\n",
    "#     else:\n",
    "#         decoded+=ds.tokenizer.decode(inp[0].unsqueeze(0))\n",
    "#     return decoded+decode_masked(inp[1:])\n",
    "    \n",
    "# print([decode_masked(inp) for inp in ds[0]])\n",
    "\n",
    "\n",
    "\n",
    "# ds = Dataset(dataset_identifier=\"race\", name=\"middle\", split=\"test\", prefix_size=5, token_permute_map=None, max_len=512, separator=\"CORRECT ANSWER: \") #permute_map)\n",
    "# print(ds[0])\n",
    "# def decode_masked(inp):\n",
    "#     if len(inp) == 0:\n",
    "#         return \"\"\n",
    "#     decoded = \"\"\n",
    "#     if inp[0] == -1:\n",
    "#         decoded+=\"[-1]\"\n",
    "#     else:\n",
    "#         decoded+=ds.tokenizer.decode(inp[0].unsqueeze(0))\n",
    "#     return decoded+decode_masked(inp[1:])\n",
    "    \n",
    "# print([decode_masked(inp) for inp in ds[0]])\n",
    "\n",
    "ds = Dataset(dataset_identifier=\"winogrande\", name=\"winogrande_debiased\", split=\"test\", prefix_size=5, token_permute_map=None, max_len=128, separator=\"CORRECT ANSWER: \") #permute_map)\n",
    "print(ds[0])\n",
    "def decode_masked(inp):\n",
    "    if len(inp) == 0:\n",
    "        return \"\"\n",
    "    decoded = \"\"\n",
    "    if inp[0] == -1:\n",
    "        decoded+=\"[-1]\"\n",
    "    else:\n",
    "        decoded+=ds.tokenizer.decode(inp[0].unsqueeze(0))\n",
    "    return decoded+decode_masked(inp[1:])\n",
    "    \n",
    "print([decode_masked(inp) for inp in ds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset winogrande (/homes/cdt21/aleks/.cache/huggingface/datasets/winogrande/winogrande_debiased/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n",
      "Found cached dataset winogrande (/homes/cdt21/aleks/.cache/huggingface/datasets/winogrande/winogrande_debiased/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n",
      "Found cached dataset winogrande (/homes/cdt21/aleks/.cache/huggingface/datasets/winogrande/winogrande_debiased/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n",
      "Found cached dataset winogrande (/homes/cdt21/aleks/.cache/huggingface/datasets/winogrande/winogrande_debiased/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n"
     ]
    }
   ],
   "source": [
    "prefix_size = 64\n",
    "# dataset_name = \"dair-ai/emotion\"; _name = None; separator = \" CLASS: \"; max_len=128\n",
    "# dataset_name = \"race\"; _name = \"middle\"; separator = \"CORRECT ANSWER: \"; max_len=512\n",
    "dataset_name = \"winogrande\"; _name = \"winogrande_debiased\"; separator = \"CORRECT ANSWER: \"; max_len=128\n",
    "batch_size = 24\n",
    "\n",
    "max_len += prefix_size\n",
    "rng = np.random.default_rng(12345)\n",
    "permute_map = {x: y for x, y in zip(range(50257), rng.permutation(range(50257)))}\n",
    "train_dataset = Dataset(dataset_identifier=dataset_name, name=_name, split=\"train\", prefix_size=prefix_size, max_len=max_len, separator=separator) \n",
    "test_dataset = Dataset(dataset_identifier=dataset_name, name=_name, split=\"test\", prefix_size=prefix_size, max_len=max_len, separator=separator)\n",
    "train_dataset_permuted = Dataset(dataset_identifier=dataset_name, name=_name, split=\"train\", prefix_size=prefix_size, token_permute_map=permute_map, max_len=max_len, separator=separator)\n",
    "test_dataset_permuted = Dataset(dataset_identifier=dataset_name, name=_name, split=\"test\", prefix_size=prefix_size, token_permute_map=permute_map, max_len=max_len, separator=separator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained model (GPT-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 354.82M\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model_name = \"gpt2-medium\"\n",
    "model = GPT.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval();\n",
    "\n",
    "# no grads\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo_callback(x):\n",
    "    batch_end_callback(x)\n",
    "    save_checkpoint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix loaded from cache.\n",
      "Performance on the winogrande dataset with prefix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset winogrande (/homes/cdt21/aleks/.cache/huggingface/datasets/winogrande/winogrande_debiased/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that '????????????????????????????????????????????????????????????????She wanted to save more money so she cut back on buying a daily magazine but not coffee because the _ was essential. What is missing: magazine or coffee? CORRECT ANSWER: ' has class ' magazine<|endoftext|>' but gt is ' coffee<|endoftext|>'\n",
      "GPT claims that '????????????????????????????????????????????????????????????????Lindsey wanted to write a letter to Betty even though she knew _ would never send it. What is missing: Lindsey or Betty? CORRECT ANSWER: ' has class ' Betty<|endoftext|>' but gt is ' Lindsey<|endoftext|>'\n",
      "GPT claims that '????????????????????????????????????????????????????????????????I tried to move the table as I had moved the chair, but had more problems with the table because the _ was lighter. What is missing: table or chair? CORRECT ANSWER: ' has class ' table<|endoftext|>' but gt is ' chair<|endoftext|>'\n",
      "Final score: 1009/1024 = 98.54% correct\n"
     ]
    }
   ],
   "source": [
    "# Train a prefix for this task\n",
    "\n",
    "fname = f'05_prefix_{model_name}_{dataset_name.replace(\"/\",\"_\")}_{prefix_size}.pth'\n",
    "remove_lora(model)\n",
    "if os.path.exists(fname):\n",
    "    prefix = torch.load(fname)\n",
    "    print(f\"Prefix loaded from cache.\")\n",
    "else:\n",
    "    prefix = torch.randn((model.config.n_layer, prefix_size, model.config.n_embd), requires_grad=True, device=device)\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.batch_size = batch_size\n",
    "    train_config.num_workers = 8\n",
    "    train_config.max_iters = 10_000\n",
    "    train_config.learning_rate = 5e-5\n",
    "    trainer = PrefixTrainer(train_config, model, train_dataset, prefix)\n",
    "    trainer.set_callback('on_batch_end', combo_callback)\n",
    "    trainer.run()\n",
    "    torch.save(prefix, fname)\n",
    "print(f\"Performance on the {dataset_name} dataset with prefix:\")\n",
    "_ = eval_classification(model, dataset=test_dataset, device=device, max_batches=1024, prefixes=prefix, show_wrong_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with the permuted dataset \n",
    "fname = f'05_prefix_{model_name}_{dataset_name.replace(\"/\",\"_\")}_{prefix_size}_permuted.pth'\n",
    "remove_lora(model)\n",
    "if os.path.exists(fname):\n",
    "    prefix_permuted = torch.load(fname)\n",
    "    print(f\"Prefix loaded from cache.\")\n",
    "else:\n",
    "    prefix_permuted  = torch.randn((model.config.n_layer, prefix_size, model.config.n_embd), requires_grad=True, device=device)\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.num_workers = batch_size\n",
    "    train_config.batch_size = 8\n",
    "    train_config.max_iters = 10_000\n",
    "    train_config.learning_rate = 5e-5\n",
    "    trainer = PrefixTrainer(train_config, model, train_dataset_permuted, prefix_permuted)\n",
    "    trainer.set_callback('on_batch_end', combo_callback)\n",
    "    trainer.run()\n",
    "    torch.save(prefix_permuted, fname)\n",
    "print(f\"Performance on the {dataset_name} dataset with prefix:\")\n",
    "_ = eval_classification(model, dataset=test_dataset_permuted, device=device, max_batches=1024, prefixes=prefix_permuted, show_wrong_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LoRA with the permuted tokens\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = batch_size\n",
    "train_config.max_iters = 5_000\n",
    "train_config.learning_rate = 5e-3\n",
    "trainer = LoRATrainer(\n",
    "    train_config, \n",
    "    model, \n",
    "    train_dataset_permuted, \n",
    "    rank=1,\n",
    "    device=device,\n",
    ")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer.run()\n",
    "_ = eval_classification(model, test_dataset_permuted, device=device, max_batches=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lora_params = sum(p.numel() for p in get_lora_state_dict(model).values())\n",
    "print(f\"Number of LoRA parameters: {n_lora_params}\")\n",
    "print(f\"Equivalent to {n_lora_params/(model.config.n_layer*model.config.n_embd)}-long prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
