{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 6.65 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm \n",
    "# add current directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), \"llama\"))\n",
    "sys.path.append(os.path.join(\"..\", \"llama\"))\n",
    "\n",
    "\n",
    "from llama import Llama\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12358\"\n",
    "os.environ[\"NUM_TRAINERS\"] = \"1\"\n",
    "\n",
    "model_name = \"llama\"\n",
    "max_seq_len = 11\n",
    "max_batch_size = 1024\n",
    "generator = Llama.build(\n",
    "        ckpt_dir=f\"../{model_name}/META_RELEASED_WEIGHTS/7B\",\n",
    "        tokenizer_path=f\"../{model_name}/META_RELEASED_WEIGHTS/tokenizer.model\",\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=max_batch_size,\n",
    "        model_parallel_size=1,\n",
    "    )\n",
    "generator.model = generator.model.half()\n",
    "for n, p in generator.model.named_parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many completions with a single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [04:34<00:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique completions: 24426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the 10-token long completions for each possible token\n",
    "batches = torch.arange(32_000).split(max_batch_size)\n",
    "completions = torch.zeros((32_000, 10), dtype=torch.long)\n",
    "\n",
    "for b_idx, b in tqdm.tqdm(enumerate(batches), total=len(batches)):\n",
    "    b = b.unsqueeze(1).to(\"cuda\")\n",
    "    for i in range(10):\n",
    "        next_tokens = generator.model.forward(b, start_pos=0)[:, -1, :].argmax(dim=-1).flatten()\n",
    "        completions[b_idx * max_batch_size : min((b_idx + 1) * max_batch_size, 32_000),i] = next_tokens\n",
    "        b = torch.cat((b, next_tokens.unsqueeze(1)), dim=1)\n",
    "\n",
    "# Find the number of unique completions\n",
    "unique_completions = torch.unique(completions, dim=0)\n",
    "print(f\"Number of unique completions: {unique_completions.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ynchronized with the 2018 FIFA World\n",
      " - tie the game at 1-1.\n",
      "The\n",
      " - traduction de l'allemand par M.\n",
      " - ia, and the United States.\n",
      "The 2\n",
      " - เראל בן יוס\n",
      " - Tasmania, Australia.\n",
      "The 201\n",
      " - building, and the like.\n",
      "The first step in\n",
      " - cosystems.\n",
      "The project is a collaboration between\n",
      " - magic.\n",
      "The first thing I noticed about the book\n",
      " - Filipino-American, and a member of the Filip\n"
     ]
    }
   ],
   "source": [
    "# decode the tokens to show some examples:\n",
    "completion_strings =  generator.tokenizer.decode(torch.cat((torch.arange(32_000).unsqueeze(1), completions), dim=1).tolist())\n",
    "for i in torch.randint(0, len(completion_strings)-1, (10,)):\n",
    "    print(f\" - {completion_strings[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many completions with a single prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48218 prefixes loaded, each of length 1\n"
     ]
    }
   ],
   "source": [
    "prefixes = torch.load(\"llama_completions.pt\", map_location=\"cuda\")\n",
    "print(f\"{prefixes.size(0)} prefixes loaded, each of length {prefixes.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [06:50<00:00,  8.54s/it]\n"
     ]
    }
   ],
   "source": [
    "batches = prefixes.split(max_batch_size)\n",
    "completions = torch.zeros((prefixes.size(0), 10), dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b_idx, b_prefix in tqdm.tqdm(enumerate(batches), total=len(batches)):\n",
    "        b_prefix = b_prefix.to(\"cuda\").half()\n",
    "        for i in range(10):\n",
    "            next_tokens = generator.model.forward(b_prefix, start_pos=0, virtual_tokens=True)[:, -1, :].argmax(dim=-1).flatten()\n",
    "            completions[b_idx * max_batch_size : b_idx * max_batch_size + b_prefix.size(0),i] = next_tokens\n",
    "            b_prefix = torch.cat((b_prefix, generator.model.tok_embeddings(next_tokens.unsqueeze(1))), dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique completions: 46812\n"
     ]
    }
   ],
   "source": [
    "# Find the number of unique completions\n",
    "unique_completions = torch.unique(completions, dim=0)\n",
    "print(f\"Number of unique completions: {unique_completions.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - panes tmp tmp tmp tmp tmp tmp tmp tmp\n",
      " - uru entry entry entry in entry in entry in in\n",
      " - Archivlink...<<<<<<<<\n",
      " - adoraheheimimimifififif\n",
      " - Cmdiriäßitschßßßßßß\n",
      " - CF tmp tmp tmp tmp tmp tmp tmp tmp tmp\n",
      " - elfistiqueistiqueistiqueistiqueistiqueistiqueistique Sistique\n",
      " - Esteahnahn\n",
      "Essay on The Great\n",
      " - ipage optim prime prime prime optim prime optim optim optim\n",
      " - ловоatreatreatrecheatreche tmp tmp tmp\n"
     ]
    }
   ],
   "source": [
    "# decode the tokens to show some examples:\n",
    "completion_strings =  generator.tokenizer.decode(completions.tolist())\n",
    "for i in torch.randint(0, len(completion_strings)-1, (10,)):\n",
    "    print(f\" - {completion_strings[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
